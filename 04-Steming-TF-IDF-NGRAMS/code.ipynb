{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "corpus = [\"Muito bom este produto\", \"Muito ruim este produto\"]\n",
    "frequencia = nltk.FreqDist(corpus)\n",
    "frequencia\n",
    "\n",
    "from nltk import tokenize\n",
    "\n",
    "frase = \"Muito bom este produto\"\n",
    "\n",
    "token_por_espaço = tokenize.WhitespaceTokenizer()\n",
    "token_frase = token_por_espaço.tokenize(frase)\n",
    "token_frase\n",
    "\n",
    "token_por_espaço = tokenize.WhitespaceTokenizer()\n",
    "token_dataset = token_por_espaço.tokenize(todas_palavras)\n",
    "frequencia = nltk.FreqDist(token_dataset)\n",
    "\n",
    "dataframe_frequencia = pd.DataFrame({\"Palavra\": list(frequencia.keys()),\n",
    "                                    \"Frequencia\": list(frequencia.values())})\n",
    "\n",
    "dataframe_frequencia.head()\n",
    "\n",
    "dataframe_frequencia.nlargest(columns = \"Frequencia\", n = 10)\n",
    "\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,8))\n",
    "ax = sns.barplot(data = dataframe_frequencia.nlargest(columns = \"Frequencia\", n = 10),\n",
    "                 x = \"Palavra\", y = \"Frequencia\", color = \"lightblue\")\n",
    "ax.set(ylabel = \"Contagem\")\n",
    "plt.show()\n",
    "\n",
    "def grafico(dados, coluna_texto, quantidade):\n",
    "  todas_palavras = ' '.join([texto for texto in dados[coluna_texto]])\n",
    "  token_frase = token_por_espaço.tokenize(todas_palavras)\n",
    "  frequencia = nltk.FreqDist(token_frase)\n",
    "  dataframe_frequencia = pd.DataFrame({\"Palavra\": list(frequencia.keys()),\n",
    "                                    \"Frequencia\": list(frequencia.values())})\n",
    "  dataframe_frequencia = dataframe_frequencia.nlargest(columns = \"Frequencia\",\n",
    "                                                       n = quantidade)\n",
    " \n",
    "  plt.figure(figsize=(12,8))\n",
    "  ax = sns.barplot(data = dataframe_frequencia,\n",
    "                  x = \"Palavra\", y = \"Frequencia\", color = \"lightblue\")\n",
    "  ax.set(ylabel = \"Contagem\")\n",
    "  plt.show()\n",
    "\n",
    "palavras_irrelevantes = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "\n",
    "frase_processada = list()\n",
    "for avaliacao in avaliacoes.review_text:\n",
    "  nova_frase = list()\n",
    "  palavras_texto = token_por_espaço.tokenize(avaliacao)\n",
    "  for palavra in palavras_texto:\n",
    "    if palavra not in palavras_irrelevantes:\n",
    "      nova_frase.append(palavra)\n",
    "  frase_processada.append(' '.join(nova_frase))\n",
    "\n",
    "avaliacoes[\"texto_sem_stopwords\"] = frase_processada\n",
    "\n",
    "avaliacoes.head()\n",
    "\n",
    "grafico(avaliacoes, \"texto_sem_stopwords\", 20)\n",
    "\n",
    "from string import punctuation\n",
    "punctuation\n",
    "\n",
    "pontuacao = list()\n",
    "for ponto in punctuation:\n",
    "  pontuacao.append(ponto)\n",
    "pontuacao\n",
    "\n",
    "pontuacao_stopwords = pontuacao + palavras_irrelevantes\n",
    "\n",
    "\n",
    "frase_processada = list()\n",
    "for avaliacao in avaliacoes.texto_sem_stopwords:\n",
    "  nova_frase = list()\n",
    "  palavras_texto = token_pontuacao.tokenize(avaliacao)\n",
    "  for palavra in palavras_texto:\n",
    "    if palavra not in pontuacao_stopwords:\n",
    "      nova_frase.append(palavra)\n",
    "  frase_processada.append(' '.join(nova_frase))\n",
    "\n",
    "avaliacoes[\"texto_sem_stopwords_e_pontuacao\"] = frase_processada\n",
    "\n",
    "grafico(avaliacoes, \"texto_sem_stopwords_e_pontuacao\", 20)\n",
    "\n",
    "!pip install unidecode\n",
    "\n",
    "sem_acentos = [unidecode.unidecode(texto) for texto in avaliacoes.texto_sem_stopwords_e_pontuacao]\n",
    "\n",
    "stopwords_sem_acento = [unidecode.unidecode(texto) for texto in pontuacao_stopwords]\n",
    "\n",
    "avaliacoes[\"texto_sem_stopwords_e_pontuacao_e_acentos\"] = sem_acentos\n",
    "\n",
    "\n",
    "frase_processada = list()\n",
    "for avaliacao in avaliacoes.texto_sem_stopwords_e_pontuacao_e_acentos:\n",
    "  nova_frase = list()\n",
    "  palavras_texto = token_pontuacao.tokenize(avaliacao)\n",
    "  for palavra in palavras_texto:\n",
    "    if palavra not in stopwords_sem_acento:\n",
    "      nova_frase.append(palavra)\n",
    "  frase_processada.append(' '.join(nova_frase))\n",
    "\n",
    "avaliacoes[\"texto_sem_stopwords_e_pontuacao_e_acentos\"] = frase_processada\n",
    "\n",
    "print(treinar_modelo(avaliacoes, \"texto_sem_stopwords_e_pontuacao_e_acentos\", \"polarity\"))\n",
    "\n",
    "word_cloud_neg(avaliacoes, \"texto_sem_stopwords_e_pontuacao_e_acentos\")\n",
    "\n",
    "word_cloud_pos(avaliacoes, \"texto_sem_stopwords_e_pontuacao_e_acentos\")\n",
    "\n",
    "frase_processada = list()\n",
    "for avaliacao in avaliacoes.texto_sem_stopwords_e_pontuacao_e_acentos:\n",
    "  nova_frase = list()\n",
    "  avaliacao = avaliacao.lower()\n",
    "  palavras_texto = token_pontuacao.tokenize(avaliacao)\n",
    "  for palavra in palavras_texto:\n",
    "    if palavra not in stopwords_sem_acento:\n",
    "      nova_frase.append(palavra)\n",
    "  frase_processada.append(' '.join(nova_frase))\n",
    "\n",
    "avaliacoes[\"texto_sem_stopwords_e_pontuacao_e_acentos_minusculo\"] = frase_processada\n",
    "\n",
    "word_cloud_neg(avaliacoes, \"texto_sem_stopwords_e_pontuacao_e_acentos_minusculo\")\n",
    "\n",
    "word_cloud_pos(avaliacoes, \"texto_sem_stopwords_e_pontuacao_e_acentos_minusculo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
